# Deployment Configuration
# This file defines deployment and serving parameters

# Serving Configuration
serving:
  # API framework
  framework: "fastapi"  # Options: fastapi, flask, django, tensorflow_serving, torchserve

  # Server configuration
  host: "0.0.0.0"
  port: 8000
  workers: 1
  timeout: 30

  # API endpoints
  endpoints:
    health: "/health"
    predict: "/predict"
    batch_predict: "/batch_predict"
    metadata: "/metadata"

  # Request/response configuration
  max_batch_size: 100
  max_request_size: "10MB"
  request_timeout: 30

# Model Serialization
model_serialization:
  # Serialization format
  format: "pickle"  # Options: pickle, joblib, onnx, saved_model, torchscript

  # Model versioning
  versioning:
    enabled: true
    version_format: "v{major}.{minor}.{patch}"
    current_version: "1.0.0"

  # Model registry
  registry:
    enabled: false
    type: "local"  # Options: local, mlflow, azure, sagemaker
    path: "models/registry"

# Container Configuration
container:
  # Docker configuration
  docker:
    enabled: true
    image_name: "generic-ai-model"
    tag: "latest"
    base_image: "python:3.9-slim"
    ports:
      - "8000:8000"

  # Kubernetes configuration
  kubernetes:
    enabled: false
    deployment_name: "generic-ai-model"
    replicas: 1
    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

# Cloud Deployment
cloud:
  # AWS configuration
  aws:
    enabled: false
    region: "us-east-1"
    sagemaker:
      endpoint_name: "generic-ai-model-endpoint"
      instance_type: "ml.t2.medium"
      initial_instance_count: 1

  # Google Cloud configuration
  gcp:
    enabled: false
    project_id: ""
    region: "us-central1"
    ai_platform:
      model_name: "generic_ai_model"
      version_name: "v1"
      framework: "tensorflow"

  # Azure configuration
  azure:
    enabled: false
    subscription_id: ""
    resource_group: ""
    workspace_name: ""
    web_service:
      name: "generic-ai-model-service"
      compute_type: "ACI"
      cpu_cores: 1
      memory_gb: 1

# Monitoring and Observability
monitoring:
  # Metrics collection
  metrics:
    enabled: true
    collection_interval: 60  # seconds
    metrics:
      - "prediction_count"
      - "prediction_latency"
      - "error_count"
      - "model_accuracy"

  # Logging
  logging:
    level: "INFO"
    format: "json"
    handlers:
      - "console"
      - "file"
    log_file: "logs/deployment.log"

  # Health checks
  health_checks:
    enabled: true
    interval: 30
    timeout: 10
    endpoints:
      - "/health"
      - "/ready"

# Security Configuration
security:
  # Authentication
  authentication:
    enabled: false
    type: "basic"  # Options: basic, oauth2, api_key, jwt
    secret_key: ""  # For JWT tokens

  # Authorization
  authorization:
    enabled: false
    roles:
      - "admin"
      - "user"

  # Input validation
  input_validation:
    enabled: true
    strict_mode: true
    allowed_content_types: ["application/json"]

  # Rate limiting
  rate_limiting:
    enabled: false
    requests_per_minute: 60
    burst_limit: 10

  # Encryption
  encryption:
    enabled: false
    key_path: ""
    algorithm: "AES256"

# Performance Optimization
performance:
  # Caching
  caching:
    enabled: true
    type: "memory"  # Options: memory, redis, memcached
    ttl: 3600  # seconds
    max_size: 1000

  # Model optimization
  model_optimization:
    enabled: false
    quantization: false
    pruning: false
    compilation: false

  # Load balancing
  load_balancing:
    enabled: false
    algorithm: "round_robin"  # Options: round_robin, least_connections, ip_hash

# Backup and Recovery
backup:
  enabled: true
  schedule: "daily"  # Options: hourly, daily, weekly
  retention_days: 30
  storage:
    type: "local"  # Options: local, s3, gcs, azure
    path: "backups"

# Scaling Configuration
scaling:
  # Auto-scaling
  auto_scaling:
    enabled: false
    min_instances: 1
    max_instances: 10
    target_cpu_utilization: 70
    target_memory_utilization: 80

  # Manual scaling
  manual_scaling:
    instances: 1

# Environment Configuration
environment:
  # Environment variables
  variables:
    MODEL_PATH: "models/production_model.pkl"
    LOG_LEVEL: "INFO"
    DEBUG_MODE: "false"

  # Configuration profiles
  profiles:
    development:
      debug: true
      log_level: "DEBUG"
    staging:
      debug: false
      log_level: "INFO"
    production:
      debug: false
      log_level: "WARNING"

# Integration Configuration
integrations:
  # Database integration
  database:
    enabled: false
    type: "postgresql"  # Options: postgresql, mysql, sqlite
    connection_string: ""

  # Message queue
  message_queue:
    enabled: false
    type: "redis"  # Options: redis, rabbitmq, sqs
    queue_name: "predictions"

  # External APIs
  external_apis:
    enabled: false
    endpoints: []

# Error Handling and Recovery
error_handling:
  # Retry configuration
  retry:
    enabled: true
    max_attempts: 3
    backoff_factor: 2
    max_delay: 60

  # Circuit breaker
  circuit_breaker:
    enabled: false
    failure_threshold: 5
    recovery_timeout: 60
    success_threshold: 3

  # Fallback responses
  fallback:
    enabled: true
    default_response: {"error": "Service temporarily unavailable"}
    cache_fallback: false

# Compliance and Governance
compliance:
  # Data governance
  data_governance:
    enabled: false
    retention_policy: "1year"
    data_classification: "internal"

  # Audit logging
  audit_logging:
    enabled: true
    log_predictions: false
    log_errors: true
    log_performance: true

  # Model explainability
  explainability:
    enabled: false
    method: "lime"  # Options: lime, shap, integrated_gradients
    save_explanations: true
